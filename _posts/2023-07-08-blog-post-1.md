---
title: 'Voice Synthesis'
date: 2023-07-08
permalink: /posts/2023/07/blog-post-0/
tags:
  - ML
  - Speech
  - NLP
---


Speech synthesis with controllable voice is a challenging task.

Recent works propose speech language models for voice cloning, and more details are described in the [blog on SpeechLM](https://hongyugong.github.io/posts/2023/07/blog-post-1/).

**FastSpeech 2(s)**

![FastSpeech 2](/images/fastspeech2.png)


**VITS** (Varitioanl Inference for Text-to-Speech)

![VITS model](/images/vits_model.png)


**Natural Speech**

![Natural Speech](/images/natural_speech.png)



**Make-A-Voice<sup>[1]</sup>**

Speech units:

(1) Semantic units by HuBERT;

(2) Acoustic tokens by Soundstream.

Training is similar to AudioLM's coarse-to-fine mechanism, but make-a-voice is built upon a seq2seq model instead of decoder-only language model.

(1) Semantic token extraction. If the input is speech in the case of voice cloning, HuBERT extracts its semantic units; if the input is text in the case of TTS, a text-to-unit transformer learns the units.

(2) Semantic-to-acoustic units. A Transformer concatenates acoustic tokens of speech prompt and semantic units of target as input, and predicts target acoustic tokens. 
In the application of singing voice synthesis, the input also includes quantized log-f0 which is extracted using YAAPT algorithm.

(3) Unit-based vocoder. Instead of using Soundstream decoder, make-a-voice trains a vocoder to synthesize speech with acoustic tokens from only 3 codebooks.

Application: Voice cloning, text-to-speech synthesis, singing voice synthesis.


**References**

Huang R, Zhang C, Wang Y, Yang D, Liu L, Ye Z, Jiang Z, Weng C, Zhao Z, Yu D. Make-A-Voice: Unified Voice Synthesis With Discrete Representation. arXiv preprint arXiv:2305.19269. 2023 May 30.

